"""
Chat Service - AI ì±—ë´‡ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ (í•¨ìˆ˜í˜• ë¦¬íŒ©í† ë§ + RAG í•˜ì´ë¸Œë¦¬ë“œ)
ë°ì´í„° ë¶„ì„(SQL) ê²½ë¡œì™€ ì§€ì‹ ê²€ìƒ‰(RAG) ê²½ë¡œë¥¼ ë¼ìš°íŒ…í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.
"""
import traceback
from typing import List, Dict, Any, Optional
from app.core import settings
from .utils.llm import call_llm
from .steps import (
    route_query,
    analyze_question,
    normalize_entities,
    # select_views, # DEPRECATED
    retrieve_data,
    retrieve_knowledge,
    format_rag_context,
    analyze_data,
    generate_response,
    generate_suggestions
)


class AIChatService:
    """
    AI ì±—ë´‡ ì„œë¹„ìŠ¤
    Query Routerë¥¼ í†µí•´ ë°ì´í„° ë¶„ì„ / ì§€ì‹ ê²€ìƒ‰ ê²½ë¡œë¥¼ ìë™ ì„ íƒí•©ë‹ˆë‹¤.
    """
    
    def __init__(self, ollama_host: str, model: str):
        self.ollama_host = ollama_host
        self.model = model
        self.conversation_history = []
        self.session_histories = {} # ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ë¡ ê°€ìƒ ì €ì¥ (DB í´ë°±ìš©)
    
    async def _get_persistent_history(self, session_id: str, limit: int = 5) -> List[Dict[str, Any]]:
        """DBì—ì„œ ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            from app.core.database import supabase
            result = supabase.table("chat_history")\
                .select("role", "content")\
                .eq("session_id", session_id)\
                .order("created_at", desc=True)\
                .limit(limit * 2)\
                .execute()
            
            # ìµœì‹  ìˆœìœ¼ë¡œ ê°€ì ¸ì™”ìœ¼ë¯€ë¡œ ë‹¤ì‹œ ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
            history = result.data[::-1] if result.data else []
            return history
        except Exception as e:
            print(f"[History] DB ì¡°íšŒ ì‹¤íŒ¨, ë©”ëª¨ë¦¬ ìºì‹œ ì‚¬ìš©: {e}")
            return self.session_histories.get(session_id, [])[-limit*2:]

    async def _save_message(self, session_id: str, role: str, content: str):
        """ëŒ€í™”ë¥¼ DBì— ì˜êµ¬ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            from app.core.database import supabase
            supabase.table("chat_history").insert({
                "session_id": session_id,
                "role": role,
                "content": content
            }).execute()
        except Exception as e:
            print(f"[History] DB ì €ì¥ ì‹¤íŒ¨: {e}")
            # ë©”ëª¨ë¦¬ ìºì‹œ ì—…ë°ì´íŠ¸
            if session_id not in self.session_histories:
                self.session_histories[session_id] = []
            self.session_histories[session_id].append({"role": role, "content": content})

    async def chat(self, user_message: str, session_id: str = "default") -> Dict[str, Any]:
        """ì±—ë´‡ ì‘ë‹µ íŒŒì´í”„ë¼ì¸ (ë³‘ë ¬í™” ë° ì˜ì†ì„± ê°•í™” ë²„ì „)"""
        
        print(f"[Chat] ========================================")
        print(f"[Chat] ì‚¬ìš©ì ì§ˆë¬¸: {user_message}")
        print(f"[Chat] ========================================")
        
        all_thinking = []
        
        try:
            # 0. íˆìŠ¤í† ë¦¬ ë¡œë“œ (ì˜ì†í™” ë¹„í™œì„±í™” - ë©”ëª¨ë¦¬ë§Œ ì‚¬ìš©)
            # history = await self._get_persistent_history(session_id)
            history = self.session_histories.get(session_id, [])[-10:]
            
            # 0.5ë‹¨ê³„: ì§ˆë¬¸ ë¼ìš°íŒ…
            print(f"[Chat] [0ë‹¨ê³„] Query ë¼ìš°íŒ… ì¤‘...")
            route_result = await route_query(user_message, self.ollama_host, self.model)
            query_route = route_result.get("route", "data")
            all_thinking.append(f"[0ë‹¨ê³„] ë¼ìš°íŒ…: {query_route} ({route_result.get('thinking', '')})")
            print(f"[Chat] [0ë‹¨ê³„] ë¼ìš°íŒ… ê²°ê³¼: {query_route}")
            
            # ============================================
            # KNOWLEDGE ê²½ë¡œ (RAG)
            # ============================================
            if query_route == "knowledge":
                print(f"[Chat] [RAG] ì§€ì‹ ê²€ìƒ‰ ê²½ë¡œ ì‹œì‘...")
                
                # RAG ê²€ìƒ‰ (AI ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ)
                rag_result = await retrieve_knowledge(user_message, self.ollama_host, self.model)
                documents = rag_result.get("documents", [])
                all_thinking.append(f"[RAG] ê²€ìƒ‰ ê²°ê³¼: {len(documents)}ê°œ ë¬¸ì„œ")
                
                if documents:
                    # ê²€ìƒ‰ëœ ë¬¸ì„œë¡œ ì‘ë‹µ ìƒì„±
                    rag_context = format_rag_context(documents)
                    rag_response = await self._generate_rag_response(user_message, rag_context)
                    all_thinking.append(f"[RAG] ì‘ë‹µ ìƒì„± ì™„ë£Œ")
                    
                    if session_id not in self.session_histories:
                        self.session_histories[session_id] = []
                    self.session_histories[session_id].append({"role": "user", "content": user_message})
                    self.session_histories[session_id].append({"role": "assistant", "content": rag_response})
                    # await self._save_message(session_id, "user", user_message)
                    # await self._save_message(session_id, "assistant", rag_response)
                    
                    return {
                        "response": rag_response,
                        "tools_used": ["knowledge_base"],
                        "session_id": session_id,
                        "response_type": "text",
                        "thinking": "\n".join(all_thinking),
                        "suggested_questions": [],
                        "insights": [],
                        "related_analyses": []
                    }
                else:
                    # RAG ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ë°ì´í„° ê²½ë¡œë¡œ í´ë°±
                    print(f"[Chat] [RAG] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ â†’ ë°ì´í„° ê²½ë¡œë¡œ í´ë°±")
                    all_thinking.append(f"[RAG] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ, ë°ì´í„° ê²½ë¡œë¡œ í´ë°±")
                    query_route = "data"
            
            # ============================================
            # DATA ê²½ë¡œ (ê¸°ì¡´ SQL íŒŒì´í”„ë¼ì¸)
            # ============================================
            # 1ë‹¨ê³„: í†µí•© ì§ˆë¬¸ ë¶„ì„ (ì§ˆë¬¸ ë¶„ì„ + View ì„ íƒ + ìŠ¤í‚¤ë§ˆ ë™ê¸°í™”)
            print(f"[Chat] [1ë‹¨ê³„] í†µí•© ì§ˆë¬¸ ë¶„ì„ ë° ìŠ¤í‚¤ë§ˆ ë¡œë“œ ì¤‘...")
            
            # Phase 2: Self-Healing Metadata (DBì—ì„œ ì‹¤ì‹œê°„ ìŠ¤í‚¤ë§ˆ ë¡œë“œ)
            from app.core.base_service import BaseService
            view_schema = await BaseService.get_ai_view_schema()
            
            last_turn_summary = ""
            if len(history) >= 2:
                last_user = history[-2].get("content", "")
                last_bot = history[-1].get("content", "")
                if last_user and last_bot:
                    last_turn_summary = f"User: {last_user}\nAssistant: {last_bot[:200]}..."

            # Phase 1: Zero-Latency (í†µí•© ë¶„ì„ í˜¸ì¶œ)
            question_analysis = await analyze_question(
                user_message, 
                self.ollama_host, 
                self.model,
                view_schema=view_schema,
                last_turn_summary=last_turn_summary
            )
            all_thinking.append(f"[1ë‹¨ê³„] {question_analysis.get('thinking', 'í†µí•© ë¶„ì„ ì™„ë£Œ')}")
            
            print(f"[Chat] [1ë‹¨ê³„] í†µí•© ë¶„ì„ ì™„ë£Œ.")


            # Short Circuit: ì¼ìƒ ëŒ€í™”
            if question_analysis.get("intent") == "conversation":
                print(f"[Chat] [Short-Circuit] ì¼ìƒ ëŒ€í™” ê°ì§€.")
                conversation_response = await self._generate_conversational_response(user_message, history)
                all_thinking.append(f"[ëŒ€í™”] ì¼ìƒ ëŒ€í™” ì²˜ë¦¬ ì™„ë£Œ")
                
                if session_id not in self.session_histories:
                    self.session_histories[session_id] = []
                self.session_histories[session_id].append({"role": "user", "content": user_message})
                self.session_histories[session_id].append({"role": "assistant", "content": conversation_response})
                # await self._save_message(session_id, "user", user_message)
                # await self._save_message(session_id, "assistant", conversation_response)

                return {
                    "response": conversation_response,
                    "tools_used": [],
                    "session_id": session_id,
                    "response_type": "text",
                    "thinking": "\n".join(all_thinking),
                    "suggested_questions": [],
                    "insights": [],
                    "related_analyses": []
                }
            
            # ============================================
            # 2ë‹¨ê³„: View ë¡œë“œ (í†µí•© ë¶„ì„ ê²°ê³¼ ì‚¬ìš©)
            # ============================================
            required_views_raw = question_analysis.get("required_views", [])
            selected_views = []
            
            if isinstance(required_views_raw, list):
                for v in required_views_raw:
                    if isinstance(v, dict) and "name" in v:
                        selected_views.append((v["name"], v.get("limit", 20)))
                    elif isinstance(v, str): # í•˜ìœ„ í˜¸í™˜
                        selected_views.append((v, 20))

            if not selected_views and question_analysis.get("intent") != "conversation":
                # ìµœí›„ì˜ ìˆ˜ë‹¨: ê¸°ë³¸ ë·°
                selected_views = [("ai_current_trending", 20)]
            
            all_thinking.append(f"[2ë‹¨ê³„] ì„ íƒëœ View: {[v[0] for v in selected_views]}")
            print(f"[Chat] [2ë‹¨ê³„] ì„ íƒëœ View: {[v[0] for v in selected_views]}")
            
            # ============================================
            # 3ë‹¨ê³„: ë°ì´í„° ì¡°íšŒ (ë³‘ë ¬ ì²˜ë¦¬)
            # ============================================
            print(f"[Chat] [3ë‹¨ê³„] ë°ì´í„° ë³‘ë ¬ ì¡°íšŒ ì‹œì‘...")
            
            filters = question_analysis.get("filters", [])
            sort = question_analysis.get("sort")

            # retrieve_dataë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ ë³‘ë ¬í™”í•˜ê±°ë‚˜ ì—¬ê¸°ì„œ ê°œë³„ í˜¸ì¶œ
            all_data, tools_used, view_summaries, data_retrieval_thinking = await retrieve_data(
                selected_views,
                filters=filters,
                sort=sort
            )
            all_thinking.append(f"[3ë‹¨ê³„] {data_retrieval_thinking}")
            
            print(f"[Chat] [3ë‹¨ê³„] ì¡°íšŒ ì™„ë£Œ: {len(all_data)}ê°œ View ì„±ê³µ")
            
            # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ì—ë„ generate_responseë¡œ ë„˜ê²¨ì„œ LLMì´ ì„¤ëª…í•˜ë„ë¡ í•¨
            if not all_data:
                print(f"[Chat] ì¡°íšŒëœ ë°ì´í„° ì—†ìŒ. LLM ì„¤ëª… ëª¨ë“œë¡œ ì „í™˜.")
                # ë¹ˆ all_dataë¥¼ ìœ ì§€í•˜ê³  ì§„í–‰
            
            # ============================================
            # 4ë‹¨ê³„: ì¢…í•© ë¶„ì„
            # ============================================
            print(f"[Chat] [4ë‹¨ê³„] ë°ì´í„° ì¢…í•© ë¶„ì„ ì¤‘...")
            comprehensive_analysis = ""
            if all_data:
                comprehensive_analysis, _ = analyze_data(all_data, user_message, question_analysis)
            all_thinking.append(f"[4ë‹¨ê³„] ì¢…í•© ë¶„ì„ ì™„ë£Œ")
            
            # ============================================
            # 5ë‹¨ê³„: ë‹µë³€ ìƒì„±
            # ============================================
            print(f"[Chat] [5ë‹¨ê³„] ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")
            result = await generate_response(
                user_message,
                all_data,
                comprehensive_analysis,
                history, # ë³€ê²½: self.conversation_history ëŒ€ì‹  history ì‚¬ìš©
                self.ollama_host,
                self.model,
                question_analysis=question_analysis
            )
            all_thinking.append(f"[5ë‹¨ê³„] {result.get('thinking', 'ë‹µë³€ ìƒì„± ì™„ë£Œ')}")
            
            # ============================================
            # 6ë‹¨ê³„: ëŠ¥ë™ì  ì œì•ˆ (ë¹„ë™ê¸°, ì‹¤íŒ¨ í—ˆìš©)
            # ============================================
            print(f"[Chat] [6ë‹¨ê³„] ëŠ¥ë™ì  ì œì•ˆ ìƒì„± ì¤‘...")
            suggestions = {}
            try:
                suggestions = await generate_suggestions(
                    user_message,
                    result["response"],
                    all_data,
                    question_analysis,
                    self.ollama_host,
                    self.model
                )
                all_thinking.append(f"[6ë‹¨ê³„] ì œì•ˆ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                print(f"[WARN] ì œì•ˆ ìƒì„± ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
                all_thinking.append(f"[6ë‹¨ê³„] ì œì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
                suggestions = {}
            
            # ëŒ€í™” ì €ì¥ (ì˜ì†í™” ë¹„í™œì„±í™” - ë©”ëª¨ë¦¬ë§Œ ì‚¬ìš©)
            if session_id not in self.session_histories:
                self.session_histories[session_id] = []
            self.session_histories[session_id].append({"role": "user", "content": user_message})
            self.session_histories[session_id].append({"role": "assistant", "content": result["response"]})
            # await self._save_message(session_id, "user", user_message)
            # await self._save_message(session_id, "assistant", result["response"])
            
            # Safe tools_used
            safe_tools_used = []
            for item in tools_used:
                if isinstance(item, tuple):
                    safe_tools_used.append(item[0])
                else:
                    safe_tools_used.append(str(item))
            
            print(f"[Chat] ì²˜ë¦¬ ì™„ë£Œ")
            print(f"[Chat] ========================================")
            
            return {
                "response": result["response"],
                "tools_used": safe_tools_used,
                "session_id": session_id,
                "response_type": result.get("response_type", "text"),
                "structured_data": result.get("structured_data"),
                "thinking": "\n".join(all_thinking),
                "suggested_questions": suggestions.get("suggested_questions", []),
                "insights": suggestions.get("insights", []),
                "related_analyses": suggestions.get("related_analyses", [])
            }
            
        except Exception as e:
            print(f"[ERROR] Chat ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {type(e).__name__}: {e}")
            traceback.print_exc()
            error_msg = str(e)
            all_thinking.append(f"[ERROR] {error_msg}")
            
            return {
                "response": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}",
                "error": error_msg,
                "session_id": session_id,
                "tools_used": [],
                "thinking": "\n".join(all_thinking)
            }
    
    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history = []
    
    async def _generate_conversational_response(self, user_message: str) -> str:
        """ë°ì´í„° ì¡°íšŒ ì—†ì´ ì¼ìƒ ëŒ€í™”ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        
        system_prompt = """ë‹¹ì‹ ì€ YouTube ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤. 
ë°ì´í„° ì¡°íšŒ ì—†ì´ ì‚¬ìš©ìì˜ ì¸ì‚¬ì— ë‹µí•˜ê±°ë‚˜, ìì‹ ì„ ì†Œê°œí•˜ê±°ë‚˜, ê°€ë²¼ìš´ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ì„¸ìš”.
í•­ìƒ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ íƒœë„ë¥¼ ìœ ì§€í•˜ì„¸ìš”. 
ë§Œì•½ ì‚¬ìš©ìê°€ ë°ì´í„°ë¥¼ ìš”êµ¬í•˜ëŠ” ì§ˆë¬¸ì„ í–ˆëŠ”ë° 'conversation'ìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆë‹¤ë©´, 
"ì£„ì†¡í•˜ì§€ë§Œ ê·¸ ì§ˆë¬¸ì€ ë°ì´í„°ë¥¼ ì¡°íšŒí•´ì•¼ ì •í™•íˆ ë‹µë³€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì‹œê² ì–´ìš”?"ë¼ê³  ì •ì¤‘íˆ ë‹µí•˜ì„¸ìš”.
í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."""

        messages = [
            {"role": "system", "content": system_prompt},
        ]
        
        messages.extend(self.conversation_history[-4:])
        messages.append({"role": "user", "content": user_message})
        
        try:
            # call_llm ìœ í‹¸ë¦¬í‹° ì‚¬ìš©
            response = await call_llm(
                self.ollama_host, 
                self.model, 
                messages, 
                temperature=0.7
            )
            return response.get("message", {}).get("content", "ì•ˆë…•í•˜ì„¸ìš”! ìœ íŠœë¸Œ ë°ì´í„° ë¶„ì„ì„ ë„ì™€ë“œë¦¬ëŠ” AIì…ë‹ˆë‹¤.")
        except Exception as e:
            print(f"[Conversational] LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            
        return "ì•ˆë…•í•˜ì„¸ìš”! ìœ íŠœë¸Œ íŠ¸ë Œë“œ ë¶„ì„ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."

    async def _generate_rag_response(self, user_message: str, rag_context: str) -> str:
        """RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        
        # ë””ë²„ê¹…: ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¶œë ¥
        print(f"[RAG Response] ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(rag_context)} chars")
        
        system_prompt = f"""ë„ˆëŠ” ìœ íŠœë¸Œ ì „ë¬¸ê°€ ì¹œêµ¬ì•¼. ì‰½ê³  ì¹œê·¼í•˜ê²Œ ì„¤ëª…í•´ì¤˜!

ì•„ë˜ [ì§€ì‹]ì„ ë°”íƒ•ìœ¼ë¡œ ë°”ë¡œ ë‹µë³€í•´. ë˜ë¬»ì§€ ë§ê³  ì•Œê³  ìˆëŠ” ë‚´ìš©ì„ ìµœëŒ€í•œ ì„¤ëª…í•´ì¤˜.

[ì§€ì‹]
{rag_context}

[ë§íˆ¬ ê·œì¹™]
- ì¹œêµ¬í•œí…Œ ì„¤ëª…í•˜ë“¯ì´ ì‰½ê²Œ ë§í•´
- ì´ëª¨ì§€ ì ì ˆíˆ ì‚¬ìš©í•´ ğŸ¯
- í•µì‹¬ë§Œ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•´
- ì „ë¬¸ ìš©ì–´ëŠ” í’€ì–´ì„œ ì„¤ëª…í•´
- ì ˆëŒ€ "ë” ìì„¸íˆ ì•Œë ¤ì£¼ì„¸ìš”", "ì–´ë–¤ ë¶€ë¶„ì´ ê¶ê¸ˆí•˜ì„¸ìš”?" ê°™ì€ ë˜ë¬»ê¸° í•˜ì§€ ë§ˆ
- ë°”ë¡œ ë³¸ë¡ ìœ¼ë¡œ ë“¤ì–´ê°€ì„œ ì„¤ëª…í•´

í•œêµ­ì–´ë¡œ ë‹µë³€í•´."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
        
        try:
            response = await call_llm(
                self.ollama_host, 
                self.model, 
                messages, 
                temperature=0.5,  # ì•½ê°„ ë” ì°½ì˜ì ìœ¼ë¡œ
                num_predict=2048  # ë” ê¸´ ì‘ë‹µ í—ˆìš©
            )
            answer = response.get("message", {}).get("content", "")
            
            if not answer or len(answer) < 20:
                print(f"[RAG Response] ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ: {answer}")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."
            
            return answer
        except Exception as e:
            print(f"[RAG Response] LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì§€ì‹ ê²€ìƒ‰ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def get_available_views(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ View ëª©ë¡ ë°˜í™˜"""
        from .views import VIEW_CATALOG
        return [
            {
                "name": view_type.value,
                "description": info.description,
                "columns": info.columns
            }
            for view_type, info in VIEW_CATALOG.items()
        ]


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_chat_service_instance = None

def get_chat_service() -> AIChatService:
    """ì±„íŒ… ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _chat_service_instance
    if _chat_service_instance is None:
        _chat_service_instance = AIChatService(
            ollama_host=settings.ollama_host,
            model=settings.ollama_model
        )
    return _chat_service_instance
